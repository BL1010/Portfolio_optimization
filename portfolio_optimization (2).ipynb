{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9a623f8",
   "metadata": {},
   "source": [
    "## INFINTIE CASH AT HAND (NO CASH REBALANCE )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "72ffdcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class PortfolioEnv_no_cash_rebalance(gym.Env):\n",
    "    def __init__(self, prices_df, window_size=20, alpha=1.0, beta=0.1):\n",
    "        super(PortfolioEnv_no_cash_rebalance, self).__init__()\n",
    "\n",
    "        self.prices = prices_df\n",
    "        self.window_size = window_size\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.current_step = self.window_size\n",
    "\n",
    "        self.num_stocks = self.prices.shape[1]\n",
    "        self.max_steps = len(self.prices) - 1\n",
    "\n",
    "        # Action: portfolio weights (with shorting allowed)\n",
    "        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(self.num_stocks,), dtype=np.float32)\n",
    "\n",
    "        # Observation: normalized window + previous weights -> flatten (window_size * num_stocks + num_stocks)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf,\n",
    "            shape=(self.window_size * self.num_stocks + self.num_stocks,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.done = False\n",
    "        self.weights = np.zeros(self.num_stocks)\n",
    "\n",
    "    def reset(self,*,seed = None,options = None):\n",
    "        super().reset(seed = seed)\n",
    "        self.current_step = self.window_size\n",
    "        self.done = False\n",
    "        self.weights = np.zeros(self.num_stocks)\n",
    "        return self._get_observation(),{}\n",
    "\n",
    "    def _get_observation(self):\n",
    "        window = self.prices.iloc[self.current_step - self.window_size:self.current_step]\n",
    "        first_day_prices = window.iloc[0].replace(0, 1e-6)\n",
    "        normalized_window = (window / first_day_prices).values.astype(np.float32)\n",
    "        flat_prices = normalized_window.flatten()\n",
    "        return np.concatenate([flat_prices, self.weights.astype(np.float32)])\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.done:\n",
    "            return self._get_observation(), 0.0, self.done,False, {}\n",
    "\n",
    "        # Normalize action to sum to 1 (portfolio constraint)\n",
    "        # action_sum = np.sum(action)\n",
    "        # if action_sum != 0:\n",
    "        #     action = action / action_sum\n",
    "        # else: #in the next training doe\n",
    "        #     action = np.ones_like(action) / len(action)\n",
    "        \n",
    "        self.weights = action\n",
    "\n",
    "        current_prices = self.prices.iloc[self.current_step]\n",
    "        previous_prices = self.prices.iloc[self.current_step - 1]\n",
    "        daily_log_returns = np.log(current_prices / previous_prices).values\n",
    "\n",
    "        portfolio_return = np.dot(self.weights, daily_log_returns)\n",
    "\n",
    "        # Portfolio risk: covariance over the window\n",
    "        window = self.prices.iloc[self.current_step - self.window_size:self.current_step]\n",
    "        log_returns = np.log(window / window.shift(1)).dropna()\n",
    "        cov_matrix = log_returns.cov().values\n",
    "        portfolio_risk = np.dot(self.weights.T, np.dot(cov_matrix, self.weights))\n",
    "\n",
    "        reward = self.alpha * portfolio_return - self.beta * portfolio_risk\n",
    "\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= self.max_steps:\n",
    "            self.done = True\n",
    "\n",
    "        obs = self._get_observation()\n",
    "        truncated = False\n",
    "\n",
    "        return obs, reward, self.done, truncated,{\n",
    "            'return': portfolio_return,\n",
    "            'risk': portfolio_risk\n",
    "        }\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(f\"Step: {self.current_step}\")\n",
    "        print(f\"Portfolio Weights: {self.weights}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0103846b",
   "metadata": {},
   "source": [
    "## ENVIRONMENT USING INITIAL CAPITAL REBALANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb14b62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class PortfolioEnv_initial_balance(gym.Env):\n",
    "    def __init__(self, prices_df, window_size=20, alpha=1.0, beta=0.1, initial_balance=10000):\n",
    "        super(PortfolioEnv_initial_balance, self).__init__()\n",
    "\n",
    "        self.prices = prices_df\n",
    "        self.window_size = window_size\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.initial_balance = initial_balance\n",
    "\n",
    "        self.current_step = self.window_size\n",
    "        self.num_stocks = self.prices.shape[1]\n",
    "        self.max_steps = len(self.prices) - 1\n",
    "\n",
    "        # Action: portfolio weights (with shorting allowed)\n",
    "        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(self.num_stocks,), dtype=np.float32)\n",
    "\n",
    "        # Observation: normalized window + previous weights + cash balance -> flatten (window_size * num_stocks + num_stocks + 1)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf,\n",
    "            shape=(self.window_size * self.num_stocks + self.num_stocks + 1,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Initialize variables\n",
    "        self.balance = self.initial_balance  # Cash balance\n",
    "        self.stock_holdings = np.zeros(self.num_stocks)  # Amount of each stock held\n",
    "        self.done = False\n",
    "        self.weights = np.zeros(self.num_stocks)\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_step = self.window_size\n",
    "        self.done = False\n",
    "        self.balance = self.initial_balance\n",
    "        self.stock_holdings = np.zeros(self.num_stocks)\n",
    "        self.weights = np.zeros(self.num_stocks)\n",
    "        \n",
    "        obs = self._get_observation()\n",
    "    # Convertion of observation to float32 explicitly to match the observation_space dtype\n",
    "        return obs.astype(np.float32), {}\n",
    "    def _get_observation(self):\n",
    "        window = self.prices.iloc[self.current_step - self.window_size:self.current_step]\n",
    "        first_day_prices = window.iloc[0].replace(0, 1e-6)\n",
    "        normalized_window = (window / first_day_prices).values.astype(np.float32)\n",
    "        flat_prices = normalized_window.flatten()\n",
    "        \n",
    "        # Return the observation (prices, weights, and balance)\n",
    "        return np.concatenate([flat_prices, self.weights.astype(np.float32), np.array([self.balance])])\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.done:\n",
    "            return self._get_observation(), 0.0, self.done, False, {}\n",
    "\n",
    "        # Normalize action to sum to 1 (portfolio constraint)\n",
    "        self.weights = action / np.sum(np.abs(action))  # Portfolio weights should sum to 1\n",
    "\n",
    "        # Determine portfolio value and transactions\n",
    "        current_prices = self.prices.iloc[self.current_step]\n",
    "        previous_prices = self.prices.iloc[self.current_step - 1]\n",
    "        daily_log_returns = np.log(current_prices / previous_prices).values\n",
    "\n",
    "        # Portfolio return calculation\n",
    "        portfolio_return = np.dot(self.weights, daily_log_returns)\n",
    "\n",
    "        # Portfolio risk: covariance over the window\n",
    "        window = self.prices.iloc[self.current_step - self.window_size:self.current_step]\n",
    "        log_returns = np.log(window / window.shift(1)).dropna()\n",
    "        cov_matrix = log_returns.cov().values\n",
    "        portfolio_risk = np.dot(self.weights.T, np.dot(cov_matrix, self.weights))\n",
    "\n",
    "        reward = self.alpha * portfolio_return - self.beta * portfolio_risk\n",
    "\n",
    "        # Portfolio update logic: buying/selling stocks\n",
    "        total_value = self.balance + np.dot(self.stock_holdings, current_prices)\n",
    "        target_stock_holdings = self.weights * total_value / current_prices  # Desired stock holdings based on weights\n",
    "\n",
    "        # Calculate transaction cost: buying/selling stocks (no short-selling restrictions)\n",
    "        transaction_cost = np.abs(target_stock_holdings - self.stock_holdings).sum() * 0.001  # 0.1% transaction fee\n",
    "        self.balance -= transaction_cost  # Deduct transaction costs from cash balance\n",
    "\n",
    "        # Update stock holdings and balance\n",
    "        for i in range(self.num_stocks):\n",
    "            cost_to_buy = (target_stock_holdings[i] - self.stock_holdings[i]) * current_prices[i]\n",
    "            if cost_to_buy > 0:  # Buying\n",
    "                if self.balance >= cost_to_buy:\n",
    "                    self.balance -= cost_to_buy\n",
    "                    self.stock_holdings[i] = target_stock_holdings[i]\n",
    "            elif cost_to_buy < 0:  # Selling\n",
    "                self.balance += np.abs(cost_to_buy)  # Return funds from selling\n",
    "                self.stock_holdings[i] = target_stock_holdings[i]\n",
    "\n",
    "        portfolio_value = self.balance + np.dot(self.stock_holdings, current_prices)\n",
    "        # The balance is already updated in the buying/selling steps above\n",
    "        # Don't reset balance, keep it as the updated amount\n",
    "\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= self.max_steps:\n",
    "            self.done = True\n",
    "\n",
    "        # Return observation, reward, done, truncated flag\n",
    "        obs = self._get_observation()\n",
    "        truncated = False\n",
    "\n",
    "        return obs.astype(np.float32), reward, self.done, truncated, {'return': portfolio_return, 'risk': portfolio_risk}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(f\"Step: {self.current_step}\")\n",
    "        print(f\"Cash Balance: {self.balance}\")\n",
    "        print(f\"Portfolio Weights: {self.weights}\")\n",
    "        print(f\"Stock Holdings: {self.stock_holdings}\")\n",
    "        print(f\"Total Portfolio Value: {self.balance + np.dot(self.stock_holdings, self.prices.iloc[self.current_step])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdd36df",
   "metadata": {},
   "source": [
    "## ENVIRONMENT WITH SHARPE RATIO REWARD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29eb091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class PortfolioEnv_sharpe(gym.Env):\n",
    "    def __init__(self, prices_df, window_size=20, alpha=1.0, beta=0.1, initial_balance=10000):\n",
    "        super(PortfolioEnv_sharpe, self).__init__()\n",
    "\n",
    "        self.prices = prices_df\n",
    "        self.window_size = window_size\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.initial_balance = initial_balance\n",
    "\n",
    "        self.current_step = self.window_size\n",
    "        self.num_stocks = self.prices.shape[1]\n",
    "        self.max_steps = len(self.prices) - 1\n",
    "\n",
    "        # Action: portfolio weights (with shorting allowed)\n",
    "        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(self.num_stocks,), dtype=np.float32)\n",
    "\n",
    "        # Observation: normalized window + previous weights + cash balance -> flatten (window_size * num_stocks + num_stocks + 1)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf,\n",
    "            shape=(self.window_size * self.num_stocks + self.num_stocks + 1,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Initialize variables\n",
    "        self.balance = self.initial_balance  # Cash balance\n",
    "        self.stock_holdings = np.zeros(self.num_stocks)  # Amount of each stock held\n",
    "        self.done = False\n",
    "        self.weights = np.zeros(self.num_stocks)\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_step = self.window_size\n",
    "        self.done = False\n",
    "        self.balance = self.initial_balance\n",
    "        self.stock_holdings = np.zeros(self.num_stocks)\n",
    "        self.weights = np.zeros(self.num_stocks)\n",
    "        \n",
    "        obs = self._get_observation()\n",
    "    # Convertion of observation to float32 explicitly to match the observation_space dtype\n",
    "        return obs.astype(np.float32), {}\n",
    "    def _get_observation(self):\n",
    "        window = self.prices.iloc[self.current_step - self.window_size:self.current_step]\n",
    "        first_day_prices = window.iloc[0].replace(0, 1e-6)\n",
    "        normalized_window = (window / first_day_prices).values.astype(np.float32)\n",
    "        flat_prices = normalized_window.flatten()\n",
    "        \n",
    "        # Return the observation (prices, weights, and balance)\n",
    "        return np.concatenate([flat_prices, self.weights.astype(np.float32), np.array([self.balance])])\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.done:\n",
    "            return self._get_observation(), 0.0, self.done, False, {}\n",
    "\n",
    "        epsilon = 1e-6\n",
    "        action = action - np.mean(action)\n",
    "        self.weights = action / (np.sum(np.abs(action)) + 1e-6)\n",
    "        # self.weights = action / (np.sum(action) + epsilon)\n",
    "\n",
    "        current_prices = self.prices.iloc[self.current_step]\n",
    "        previous_prices = self.prices.iloc[self.current_step - 1]\n",
    "        daily_log_returns = np.log(current_prices / previous_prices).values\n",
    "\n",
    "        # Compute portfolio return\n",
    "        portfolio_return = np.dot(self.weights, daily_log_returns)\n",
    "\n",
    "        # Get windowed returns to compute rolling std dev\n",
    "        window = self.prices.iloc[self.current_step - self.window_size:self.current_step]\n",
    "        log_returns = np.log(window / window.shift(1)).dropna()\n",
    "        portfolio_returns_window = np.dot(log_returns.values, self.weights)\n",
    "\n",
    "        # Calculate rolling std deviation (risk)\n",
    "        std_dev = np.std(portfolio_returns_window,ddof = 1) + 1e-6\n",
    "\n",
    "        # Sharpe ratio reward (risk-free rate assumed 0 for simplicity)\n",
    "        reward = portfolio_return / std_dev\n",
    "\n",
    "        # Rebalancing logic\n",
    "        total_value = self.balance + np.dot(self.stock_holdings, current_prices)\n",
    "        target_stock_holdings = self.weights * total_value / current_prices\n",
    "\n",
    "        transaction_cost = np.abs(target_stock_holdings - self.stock_holdings).sum() * 0.001\n",
    "        self.balance -= transaction_cost\n",
    "\n",
    "        for i in range(self.num_stocks):\n",
    "            cost_to_buy = (target_stock_holdings[i] - self.stock_holdings[i]) * current_prices[i]\n",
    "            if cost_to_buy > 0:\n",
    "                if self.balance >= cost_to_buy:\n",
    "                    self.balance -= cost_to_buy\n",
    "                    self.stock_holdings[i] = target_stock_holdings[i]\n",
    "            elif cost_to_buy < 0:\n",
    "                self.balance += np.abs(cost_to_buy)\n",
    "                self.stock_holdings[i] = target_stock_holdings[i]\n",
    "\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= self.max_steps:\n",
    "            self.done = True\n",
    "\n",
    "        obs = self._get_observation()\n",
    "        truncated = False\n",
    "        return obs.astype(np.float32), reward, self.done, truncated, {\n",
    "            'return': portfolio_return,\n",
    "            'rolling_std': std_dev\n",
    "        }\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(f\"Step: {self.current_step}\")\n",
    "        print(f\"Cash Balance: {self.balance}\")\n",
    "        print(f\"Portfolio Weights: {self.weights}\")\n",
    "        print(f\"Stock Holdings: {self.stock_holdings}\")\n",
    "        print(f\"Total Portfolio Value: {self.balance + np.dot(self.stock_holdings, self.prices.iloc[self.current_step])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da2f05d",
   "metadata": {},
   "source": [
    "## Downloading the Data ( 50 companies with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7f3094a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  10 of 10 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2923, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "# Define the stock tickers and the date range\n",
    "stock_tickers = [\n",
    "    'AAPL', 'GOOGL', 'MSFT', 'AMZN', 'TSLA',\n",
    "    'META', 'NVDA', 'INTC', 'NFLX', 'IBM'\n",
    "]\n",
    "start_date = '2007-01-01'\n",
    "end_date = '2023-12-31'\n",
    "\n",
    "# Download the data from Yahoo Finance\n",
    "stock_data = yf.download(stock_tickers, start=start_date, end=end_date)['Close']\n",
    "\n",
    "# Show the first few rows of the dataset\n",
    "stock_data.head()\n",
    "\n",
    "# Clean up the data by removing rows with NaN values\n",
    "stock_data.dropna(inplace=True)\n",
    "\n",
    "# Print the shape of the dataset\n",
    "print(stock_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cfe82b",
   "metadata": {},
   "source": [
    "## TRAINING LOOP OF PPO WITH CASH REBALANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc82757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress deprecation warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines3 import PPO, DDPG, TD3\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1) Custom Callback with Early Stopping\n",
    "# -------------------------------------------------------------------\n",
    "class LoggingCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0, reward_patience=50, epsilon=1e-2, print_freq=10):\n",
    "        super(LoggingCallback, self).__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self._current_ep_reward = 0.0\n",
    "        self.actor_losses = []\n",
    "        self.critic_losses = []\n",
    "\n",
    "        # Early stopping parameters\n",
    "        self.reward_patience = reward_patience\n",
    "        self.epsilon = epsilon\n",
    "        self.best_mean_reward = -np.inf\n",
    "        self.episodes_without_improvement = 0\n",
    "\n",
    "        # Print frequency\n",
    "        self.print_freq = print_freq\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        reward = float(self.locals[\"rewards\"][0])\n",
    "        self._current_ep_reward += reward\n",
    "\n",
    "        if self.locals[\"dones\"][0]:\n",
    "            self.episode_rewards.append(self._current_ep_reward)\n",
    "\n",
    "            # Only print logs after a certain number of episodes\n",
    "            if len(self.episode_rewards) % self.print_freq == 0:\n",
    "                print(f\"Episode {len(self.episode_rewards)} reward: {self._current_ep_reward:.4f}\")\n",
    "\n",
    "            # Early stopping logic\n",
    "            if len(self.episode_rewards) >= self.reward_patience:\n",
    "                recent_rewards = self.episode_rewards[-self.reward_patience:]\n",
    "                mean_recent = np.mean(recent_rewards)\n",
    "\n",
    "                if mean_recent > self.best_mean_reward + self.epsilon:\n",
    "                    self.best_mean_reward = mean_recent\n",
    "                    self.episodes_without_improvement = 0\n",
    "                else:\n",
    "                    self.episodes_without_improvement += 1\n",
    "                    print(f\"No reward improvement for {self.episodes_without_improvement} episodes\")\n",
    "\n",
    "                    # # Early stopping condition met\n",
    "                    # if self.episodes_without_improvement >= self.reward_patience:\n",
    "                    #     print(\"Early stopping triggered: No improvement for 50 episodes.\")\n",
    "                    #     return False  # This will stop training\n",
    "\n",
    "            self._current_ep_reward = 0.0\n",
    "\n",
    "        # Capture losses (if available in logs)\n",
    "        logs = self.logger.name_to_value\n",
    "        if \"train/actor_loss\" in logs and \"train/critic_loss\" in logs:\n",
    "            self.actor_losses.append(logs[\"train/actor_loss\"])\n",
    "            self.critic_losses.append(logs[\"train/critic_loss\"])\n",
    "\n",
    "        return True\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2) Environment\n",
    "# -------------------------------------------------------------------\n",
    "# Assume `stock_data` is your 2923x10 price DataFrame and PortfolioEnv is already defined\n",
    "env = PortfolioEnv_initial_balance(prices_df=stock_data, window_size=20)\n",
    "check_env(env, warn=True)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3) Instantiate Model\n",
    "# -------------------------------------------------------------------\n",
    "n_actions = env.action_space.shape[-1]\n",
    "\n",
    "# Add noise to actions using NormalActionNoise (mean=0, sigma=0.05)\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.05 * np.ones(n_actions))\n",
    "\n",
    "callback = LoggingCallback(reward_patience=50, epsilon=1e-2)\n",
    "\n",
    "# Instantiate PPO model\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",            # Policy type\n",
    "    env,                    # Environment\n",
    "    learning_rate=1e-4,     # Learning rate\n",
    "   # Adding action noise here\n",
    "    gamma = 0.99,\n",
    "\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4) Train\n",
    "# -------------------------------------------------------------------\n",
    "model.learn(total_timesteps=2000000, callback=callback)\n",
    "model.save(\"PPO_portfolio_trained\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275d2c88",
   "metadata": {},
   "source": [
    "## TRAINING LOOP OF DDPG USING CASH REBALANCE ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8d1420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1) Custom Callback with Early Stopping\n",
    "# -------------------------------------------------------------------\n",
    "class LoggingCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0, reward_patience=20, epsilon=1e-2):\n",
    "        super(LoggingCallback, self).__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self._current_ep_reward = 0.0\n",
    "        self.actor_losses = []\n",
    "        self.critic_losses = []\n",
    "\n",
    "        # Early stopping parameters\n",
    "        self.reward_patience = reward_patience\n",
    "        self.epsilon = epsilon\n",
    "        self.best_mean_reward = -np.inf\n",
    "        self.episodes_without_improvement = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        reward = float(self.locals[\"rewards\"][0])\n",
    "        self._current_ep_reward += reward\n",
    "\n",
    "        if self.locals[\"dones\"][0]:\n",
    "            self.episode_rewards.append(self._current_ep_reward)\n",
    "            print(f\"Episode {len(self.episode_rewards)} reward: {self._current_ep_reward:.4f}\")\n",
    "\n",
    "            # Early stopping logic\n",
    "            if len(self.episode_rewards) >= self.reward_patience:\n",
    "                recent_rewards = self.episode_rewards[-self.reward_patience:]\n",
    "                mean_recent = np.mean(recent_rewards)\n",
    "\n",
    "                if mean_recent > self.best_mean_reward + self.epsilon:\n",
    "                    self.best_mean_reward = mean_recent\n",
    "                    self.episodes_without_improvement = 0\n",
    "                else:\n",
    "                    self.episodes_without_improvement += 1\n",
    "                    print(f\"No reward improvement for {self.episodes_without_improvement} episodes\")\n",
    "\n",
    "                # if self.episodes_without_improvement >= self.reward_patience:\n",
    "                #     print(f\"Early stopping triggered after {len(self.episode_rewards)} episodes!\")\n",
    "                #     return False  # Stop training\n",
    "\n",
    "            self._current_ep_reward = 0.0\n",
    "\n",
    "        # Capture losses (if available in logs)\n",
    "        logs = self.logger.name_to_value\n",
    "        if \"train/actor_loss\" in logs and \"train/critic_loss\" in logs:\n",
    "            self.actor_losses.append(logs[\"train/actor_loss\"])\n",
    "            self.critic_losses.append(logs[\"train/critic_loss\"])\n",
    "\n",
    "        return True\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2) Environment\n",
    "# -------------------------------------------------------------------\n",
    "# Assume `stock_data` is your 2923x10 price DataFrame and PortfolioEnv is already defined\n",
    "env = PortfolioEnv_initial_balance(prices_df=stock_data, window_size=20)\n",
    "check_env(env, warn=True)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3) Instantiate Model\n",
    "# -------------------------------------------------------------------\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.05 * np.ones(n_actions))\n",
    "\n",
    "callback = LoggingCallback(reward_patience=20, epsilon=1e-2)\n",
    "\n",
    "model = DDPG(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    learning_rate=1e-4,\n",
    "    action_noise=action_noise,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4) Train\n",
    "# -------------------------------------------------------------------\n",
    "model.learn(total_timesteps=1000000, callback=callback)\n",
    "model.save(\"DDPG3_portfolio_trained\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c8b773",
   "metadata": {},
   "source": [
    "## TRAINING LOOP OF DDPG WITHOUT CASH REBALANCE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68e787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1) Custom Callback with Early Stopping\n",
    "# -------------------------------------------------------------------\n",
    "class LoggingCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0, reward_patience=20, epsilon=1e-2):\n",
    "        super(LoggingCallback, self).__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self._current_ep_reward = 0.0\n",
    "        self.actor_losses = []\n",
    "        self.critic_losses = []\n",
    "\n",
    "        # Early stopping parameters\n",
    "        self.reward_patience = reward_patience\n",
    "        self.epsilon = epsilon\n",
    "        self.best_mean_reward = -np.inf\n",
    "        self.episodes_without_improvement = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        reward = float(self.locals[\"rewards\"][0])\n",
    "        self._current_ep_reward += reward\n",
    "\n",
    "        if self.locals[\"dones\"][0]:\n",
    "            self.episode_rewards.append(self._current_ep_reward)\n",
    "            print(f\"Episode {len(self.episode_rewards)} reward: {self._current_ep_reward:.4f}\")\n",
    "\n",
    "            # Early stopping logic\n",
    "            if len(self.episode_rewards) >= self.reward_patience:\n",
    "                recent_rewards = self.episode_rewards[-self.reward_patience:]\n",
    "                mean_recent = np.mean(recent_rewards)\n",
    "\n",
    "                if mean_recent > self.best_mean_reward + self.epsilon:\n",
    "                    self.best_mean_reward = mean_recent\n",
    "                    self.episodes_without_improvement = 0\n",
    "                else:\n",
    "                    self.episodes_without_improvement += 1\n",
    "                    print(f\"No reward improvement for {self.episodes_without_improvement} episodes\")\n",
    "\n",
    "                # if self.episodes_without_improvement >= self.reward_patience:\n",
    "                #     print(f\"Early stopping triggered after {len(self.episode_rewards)} episodes!\")\n",
    "                #     return False  # Stop training\n",
    "\n",
    "            self._current_ep_reward = 0.0\n",
    "\n",
    "        # Capture losses (if available in logs)\n",
    "        logs = self.logger.name_to_value\n",
    "        if \"train/actor_loss\" in logs and \"train/critic_loss\" in logs:\n",
    "            self.actor_losses.append(logs[\"train/actor_loss\"])\n",
    "            self.critic_losses.append(logs[\"train/critic_loss\"])\n",
    "\n",
    "        return True\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2) Environment\n",
    "# -------------------------------------------------------------------\n",
    "# Assume `stock_data` is your 2923x10 price DataFrame and PortfolioEnv is already defined\n",
    "env = PortfolioEnv_no_cash_rebalance(prices_df=stock_data, window_size=20)\n",
    "check_env(env, warn=True)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3) Instantiate Model\n",
    "# -------------------------------------------------------------------\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.05 * np.ones(n_actions))\n",
    "\n",
    "callback = LoggingCallback(reward_patience=20, epsilon=1e-2)\n",
    "\n",
    "model = DDPG(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    learning_rate=1e-4,\n",
    "    action_noise=action_noise,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4) Train\n",
    "# -------------------------------------------------------------------\n",
    "model.learn(total_timesteps=1000000, callback=callback)\n",
    "model.save(\"DDPG2_portfolio_trained\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8efa413",
   "metadata": {},
   "source": [
    "## EVALUATION WITHOUT CASH REBALANCE USING DDPG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994fa77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "window_size = 20\n",
    "\n",
    "# --- Load trained model ---\n",
    "model = DDPG.load(\"DDPG2_portfolio_trained\")\n",
    "\n",
    "# --- Create evaluation environment ---\n",
    "eval_env = DummyVecEnv([lambda: PortfolioEnv_no_cash_rebalance(prices_df=stock_data, window_size=window_size)])\n",
    "obs = eval_env.reset()\n",
    "rl_returns = []\n",
    "done = [False]\n",
    "\n",
    "while not done[0]:\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, _ = eval_env.step(action)\n",
    "    rl_returns.append(reward[0])  # SB3 gives list of rewards from vecenv\n",
    "\n",
    "# --- 1/N Equal Weight Strategy ---\n",
    "equal_weights = np.ones(stock_data.shape[1]) / stock_data.shape[1]\n",
    "equal_returns = (stock_data.pct_change().dropna() @ equal_weights).cumsum()\n",
    "\n",
    "# --- Mean-Variance Strategy ---\n",
    "mean_returns = stock_data.pct_change().dropna().mean()\n",
    "cov_matrix = stock_data.pct_change().dropna().cov()\n",
    "\n",
    "inv_cov = np.linalg.pinv(cov_matrix.values)\n",
    "weights_mv = inv_cov @ mean_returns.values\n",
    "weights_mv /= weights_mv.sum()\n",
    "\n",
    "mv_returns = (stock_data.pct_change().dropna() @ weights_mv).cumsum()\n",
    "\n",
    "# --- RL strategy returns ---\n",
    "rl_returns = np.cumsum(rl_returns)\n",
    "\n",
    "# --- Plot all ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(equal_returns.index, equal_returns, label='1/N Strategy')\n",
    "plt.plot(mv_returns.index, mv_returns, label='Mean-Variance Strategy')\n",
    "plt.plot(equal_returns.index[:len(rl_returns)], rl_returns, label='DDPG Strategy')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Cumulative Returns\")\n",
    "plt.title(\"Portfolio Strategies Comparison\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3533d8",
   "metadata": {},
   "source": [
    "## EVALUATION WITH CASH RABALANCE (PPO MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287e6e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "\n",
    "window_size = 20\n",
    "\n",
    "# --- Load trained model ---\n",
    "model = PPO.load(\"PPO_portfolio_trained\")\n",
    "\n",
    "# --- Create evaluation environment ---\n",
    "eval_env = DummyVecEnv([lambda: PortfolioEnv_initial_balance(prices_df=stock_data, window_size=window_size)])\n",
    "obs = eval_env.reset()\n",
    "rl_returns = []\n",
    "done = [False]\n",
    "\n",
    "while not done[0]:\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, _ = eval_env.step(action)\n",
    "    rl_returns.append(reward[0])  # SB3 gives list of rewards from vecenv\n",
    "\n",
    "# --- 1/N Equal Weight Strategy ---\n",
    "equal_weights = np.ones(stock_data.shape[1]) / stock_data.shape[1]\n",
    "equal_returns = (stock_data.pct_change().dropna() @ equal_weights).cumsum()\n",
    "\n",
    "# --- Mean-Variance Strategy ---\n",
    "mean_returns = stock_data.pct_change().dropna().mean()\n",
    "cov_matrix = stock_data.pct_change().dropna().cov()\n",
    "\n",
    "inv_cov = np.linalg.pinv(cov_matrix.values)\n",
    "weights_mv = inv_cov @ mean_returns.values\n",
    "weights_mv /= weights_mv.sum()\n",
    "\n",
    "mv_returns = (stock_data.pct_change().dropna() @ weights_mv).cumsum()\n",
    "\n",
    "# --- RL strategy returns ---\n",
    "rl_returns = np.cumsum(rl_returns)\n",
    "\n",
    "# --- Plot all ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(equal_returns.index, equal_returns, label='1/N Strategy')\n",
    "plt.plot(mv_returns.index, mv_returns, label='Mean-Variance Strategy')\n",
    "plt.plot(equal_returns.index[:len(rl_returns)], rl_returns, label='PPO Strategy')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Cumulative Returns\")\n",
    "plt.title(\"Portfolio Strategies Comparison\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
